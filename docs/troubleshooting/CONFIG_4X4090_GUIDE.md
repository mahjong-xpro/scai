# 4x4090 训练配置指南

## 配置概述

本配置针对 **4张 NVIDIA RTX 4090 (24GB each)** 和 **0基础数据** 进行了全面优化。

### 关键优化点

1. **硬件资源充分利用**
   - 批次大小：8192（充分利用4090的24GB显存）
   - Worker数量：40（充分利用多核CPU）
   - 缓冲区：200000（存储更多经验）

2. **0基础数据支持**
   - ✅ 课程学习：必须启用，从定缺阶段开始
   - ✅ 喂牌机制：必须启用，20%喂牌帮助学习
   - ✅ 奖励配置：通过课程学习自动配置

3. **训练效率优化**
   - 收集间隔：1（每次迭代都收集）
   - 保存间隔：50（更频繁保存）
   - 评估间隔：50（更频繁评估）

## 配置详解

### 1. 模型配置

```yaml
model:
  backbone:
    num_blocks: 20             # 4090显存充足，保持20层
    channels: 128              # 基础通道数
```

**说明**：4090显存充足，可以支持更深的网络，但初期20层已经足够。

### 2. 训练配置

```yaml
training:
  batch_size: 8192             # ⬆️ 从4096增加到8192（4090显存充足）
  buffer_capacity: 200000     # ⬆️ 从100000增加到200000（存储更多经验）
  num_iterations: 5000         # ⬆️ 从1000增加到5000（0基础需要更多迭代）
  save_interval: 50            # ⬆️ 从100减少到50（更频繁保存）
```

**说明**：
- **批次大小8192**：4090每张24GB显存，可以支持更大的批次，提高训练效率
- **缓冲区200000**：存储更多经验，有利于长期学习
- **迭代次数5000**：0基础需要更多迭代完成多个训练阶段
- **保存间隔50**：更频繁保存，防止意外中断丢失进度

### 3. 自对弈配置

```yaml
selfplay:
  num_workers: 40              # ⬆️ 从10增加到40（提高数据收集速度）
  games_per_worker: 50         # ⬇️ 从100减少到50（提高并行度）
```

**说明**：
- **40个Worker**：充分利用多核CPU，提高数据收集速度
- **50游戏/Worker**：降低单worker游戏数，提高并行度，减少单worker失败影响

### 4. 课程学习配置（关键）

```yaml
curriculum_learning:
  enabled: true                 # ✅ 必须启用
  initial_stage: declare_suit   # 从定缺阶段开始
  feeding_games:
    enabled: true               # ✅ 必须启用
    feeding_rate: 0.2          # 20%喂牌，80%随机
```

**说明**：
- **课程学习必须启用**：解决0基础数据问题，自动配置奖励
- **喂牌机制必须启用**：帮助AI学习基本胡牌
- **20%喂牌率**：平衡学习和真实对局

### 5. GPU配置

```yaml
gpu:
  device_ids: [0, 1, 2, 3]     # 使用全部4张4090
ray:
  num_gpus: 4                  # 明确指定4张
```

**说明**：
- **4张GPU**：主要用于模型训练
- **Workers在CPU上运行**：4090显存充足，workers不需要GPU

## 训练流程

### 阶段1：定缺与生存（2000-8000次迭代）

**目标**：
- 理解定缺规则
- 学会选择定缺花色
- 避免成为花猪
- 掌握弃牌逻辑

**奖励配置**：
- `lack_color_discard: 5.0`（打缺门牌奖励）
- `illegal_action_attempt: -10.0`（非法动作惩罚）
- `base_win: 0.0`（不给胡牌奖励）
- `ready_reward: 0.0`（不给听牌奖励）

**预期结果**：
- 花猪率 < 50%
- 定缺选择正确率 > 60%

### 阶段2：学胡基础（4000-12000次迭代）

**目标**：
- 识别基本胡牌类型
- 理解听牌概念
- 学会主动胡牌
- 利用向听数改善手牌

**奖励配置**：
- `shanten_decrease: 2.0`（向听数减少奖励）
- `shanten_increase: -1.5`（向听数增加惩罚）
- `ready_hand: 10.0`（听牌一次性重奖）
- `feeding_rate: 0.2`（20%喂牌）

**预期结果**：
- 胜率 > 20%
- 听牌率 > 30%
- 学会2种胡牌类型

### 阶段3：价值收割（8000-20000次迭代）

**目标**：
- 识别高收益牌型
- 理解番数计算
- 学会识别根（Gen）

**奖励配置**：
- `base_win: 20.0`（基础胡牌奖励）
- `fan_multiplier: 1.5`（番数倍数）
- `gen_reward: 5.0`（根奖励）
- `feeding_rate: 0.1`（降低到10%）

**预期结果**：
- 胜率 > 15%
- 平均番数 > 2番
- 平均每局 > 0.3个根

## 监控和调试

### 1. Web仪表板

访问 `http://localhost:5000` 查看：
- 训练进度
- 奖励配置
- 性能指标
- 课程学习阶段

### 2. 日志文件

查看 `./logs/` 目录：
- 训练日志
- 评估结果
- 错误信息

### 3. Checkpoint

检查 `./checkpoints/` 目录：
- 每50次迭代保存一次
- 可以恢复训练

## 常见问题

### Q1: GPU显存不足？

**A**: 如果显存不足，可以：
- 降低 `batch_size` 到 4096 或 2048
- 减少 `num_workers` 到 20
- 检查是否有其他进程占用GPU

### Q2: Worker创建失败？

**A**: 检查：
- Ray是否正确初始化
- CPU核心数是否足够（建议至少40核）
- 内存是否充足（建议至少64GB）

### Q3: 所有奖励都是0？

**A**: 确保：
- `curriculum_learning.enabled: true`
- `feeding_games.enabled: true`
- 检查课程学习是否正常推进

### Q4: 训练速度慢？

**A**: 优化：
- 增加 `num_workers`（但不要超过CPU核心数）
- 检查网络延迟（如果使用分布式）
- 检查磁盘IO（checkpoint保存可能影响）

## 性能预期

### 硬件利用率

- **GPU利用率**：80-95%（训练时）
- **CPU利用率**：60-80%（数据收集时）
- **内存使用**：40-60GB（取决于缓冲区大小）

### 训练速度

- **数据收集**：40 workers × 50 games ≈ 2000 games/迭代
- **训练速度**：约 2-5 分钟/迭代（取决于批次大小）
- **总训练时间**：约 170-420 小时（5000次迭代）

## 优化建议

### 初期（阶段1-2）

1. **关注奖励配置**：确保课程学习正常推进
2. **监控花猪率**：如果太高，可能需要调整奖励
3. **检查喂牌效果**：确保AI能学会基本胡牌

### 中期（阶段3-4）

1. **关注胜率**：如果胜率不提升，可能需要调整学习率
2. **监控番数**：确保AI学会识别高收益牌型
3. **检查根数**：确保AI学会识别根

### 后期（阶段5-6）

1. **关注Elo评分**：评估AI整体水平
2. **对比历史模型**：确保新模型确实更好
3. **调整超参数**：根据实际情况微调

## 总结

本配置针对4x4090和0基础数据进行了全面优化：

1. ✅ **硬件资源充分利用**：批次8192，40 workers
2. ✅ **0基础数据支持**：课程学习+喂牌机制
3. ✅ **训练效率优化**：频繁收集、保存、评估
4. ✅ **监控和调试**：Web仪表板+详细日志

**预期效果**：
- 阶段1：2000-8000次迭代，学会定缺和弃牌
- 阶段2：4000-12000次迭代，学会基本胡牌
- 阶段3：8000-20000次迭代，学会高收益牌型
- 总训练时间：约170-420小时（5000次迭代）

