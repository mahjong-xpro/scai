# Dual-ResNet 模型架构详解

## 概述

Dual-ResNet 是一个用于血战到底麻将AI的深度强化学习模型，采用**双头架构**（Dual-Head Architecture）：
- **Policy Head（策略头）**：输出动作概率分布，决定AI应该执行什么动作
- **Value Head（价值头）**：输出期望收益分数，评估当前局面的价值

## 整体架构

```
输入: 游戏状态张量 (batch_size, 64, 4, 9)
    ↓
┌─────────────────────────────────────┐
│   ResNet Backbone (骨干网络)        │
│   - 初始卷积层                      │
│   - 20个残差块                      │
│   - 全局平均池化                    │
│   - 全连接层                        │
└─────────────────────────────────────┘
    ↓
特征向量 (batch_size, 512)
    ↓
    ├─────────────────┬─────────────────┐
    ↓                 ↓                 ↓
┌──────────┐    ┌──────────┐    ┌──────────┐
│ Policy   │    │ Value    │    │ (共享)   │
│ Head     │    │ Head     │    │          │
└──────────┘    └──────────┘    └──────────┘
    ↓                 ↓
动作概率分布      期望收益分数
(batch, 434)     (batch, 1)
```

## 配置参数详解

### 1. `input_channels: 64` - 输入特征平面数

**作用**：定义输入张量的通道数（特征平面数）

**说明**：
- 输入形状：`(batch_size, 64, 4, 9)`
- `64` 表示有 64 个不同的特征平面，每个平面编码不同的游戏信息
- 每个特征平面是 `4×9` 的二维数组，表示：
  - `4`：4个玩家
  - `9`：每种花色的9种牌（万、筒、条各9种）

**特征平面示例**：
- 平面 0-3：每个玩家的手牌分布
- 平面 4-7：每个玩家的已碰/杠牌
- 平面 8-11：每个玩家的弃牌历史
- 平面 12-15：定缺信息
- 平面 16-19：听牌状态
- ...（共64个平面，编码完整的游戏状态）

**为什么是64？**
- 需要编码的信息包括：手牌、碰/杠、弃牌、定缺、听牌、杠钱、过胡记录等
- 64个平面提供了足够的信息表示能力
- 可以根据需要调整，但需要同步修改特征提取代码

### 2. `action_space_size: 434` - 动作空间大小

**作用**：定义AI可以执行的所有可能动作的数量

**说明**：
- 输出形状：`(batch_size, 434)`
- 434个动作包括：
  - **索引 0-107**：出牌动作（108种牌：3种花色 × 9种数字 × 4张）
  - **索引 108-215**：碰牌动作（108种牌）
  - **索引 216-323**：杠牌动作（108种牌）
  - **索引 324-431**：胡牌动作（108种牌）
  - **索引 432**：摸牌动作
  - **索引 433**：过（放弃）动作

**为什么是434？**
- 血战到底的完整动作空间
- 每个动作类型都需要覆盖所有可能的牌
- 434 = 108×4（出/碰/杠/胡）+ 1（摸）+ 1（过）

### 3. `backbone` - 骨干网络配置

#### 3.1 `num_blocks: 20` - 残差块数量

**作用**：定义ResNet骨干网络的深度

**说明**：
- 20个残差块（ResidualBlock）组成深层特征提取网络
- 每个残差块包含：
  - 2个卷积层（Conv2d）
  - 2个批归一化层（BatchNorm2d）
  - ReLU激活函数
  - 残差连接（Residual Connection）

**残差块结构**：
```
输入 x
    ↓
Conv2d → BatchNorm → ReLU
    ↓
Conv2d → BatchNorm
    ↓
    + (残差连接)
    ↓
ReLU → 输出
```

**为什么是20层？**
- **深度与性能**：更深的网络可以学习更复杂的特征表示
- **训练稳定性**：残差连接解决了深层网络的梯度消失问题
- **计算资源**：4090显存充足，可以支持20层
- **经验值**：在AlphaGo Zero等成功案例中，20-40层是常见选择

**调整建议**：
- **减少层数**（如10-15层）：训练更快，但可能表达能力不足
- **增加层数**（如30-40层）：表达能力更强，但训练更慢，需要更多显存

#### 3.2 `channels: 128` - 基础通道数

**作用**：定义骨干网络的初始通道数（特征图深度）

**说明**：
- 初始卷积层将输入从 `64` 通道扩展到 `128` 通道
- 随着网络加深，通道数会逐步增加：
  - 第1组（块1-5）：128通道
  - 第2组（块6-10）：256通道
  - 第3组（块11-15）：512通道
  - 第4组（块16-20）：512通道（不再增加）

**通道数变化**：
```
输入: 64 通道
    ↓
初始卷积: 64 → 128 通道
    ↓
残差块组1: 128 通道（块1-5）
    ↓
残差块组2: 256 通道（块6-10）
    ↓
残差块组3: 512 通道（块11-15）
    ↓
残差块组4: 512 通道（块16-20）
```

**为什么是128？**
- **计算效率**：128是一个平衡点，既能提取丰富特征，又不会过度消耗显存
- **特征表示**：从64通道扩展到128通道，提供了足够的特征表示空间
- **4090优化**：4090显存充足，128通道可以充分利用GPU资源

**调整建议**：
- **减少通道数**（如64或96）：节省显存，但可能特征表示不足
- **增加通道数**（如256或512）：更强的特征表示，但需要更多显存和计算

### 4. `policy_head` - 策略头配置

#### 4.1 `hidden_size: 512` - 隐藏层大小

**作用**：定义策略头的隐藏层维度

**说明**：
- 策略头结构：
  ```
  特征向量 (512维)
      ↓
  全连接层1: 512 → 256
      ↓
  ReLU + Dropout(0.1)
      ↓
  全连接层2: 256 → 256
      ↓
  ReLU + Dropout(0.1)
      ↓
  全连接层3: 256 → 434
      ↓
  Softmax
      ↓
  动作概率分布 (434维)
  ```

**为什么是512？**
- **与特征维度匹配**：输入特征维度是512，隐藏层512可以保留所有信息
- **表达能力**：512维的隐藏层提供了足够的表达能力来学习复杂的动作策略
- **计算效率**：512是一个常见的隐藏层大小，在性能和效率之间取得平衡

**注意**：配置中的 `hidden_size: 512` 实际上对应代码中的 `feature_dim`，而策略头内部的隐藏层是 `hidden_dim: 256`（在代码中硬编码）。

### 5. `value_head` - 价值头配置

#### 5.1 `hidden_size: 256` - 隐藏层大小

**作用**：定义价值头的隐藏层维度

**说明**：
- 价值头结构：
  ```
  特征向量 (512维)
      ↓
  全连接层1: 512 → 256
      ↓
  ReLU + Dropout(0.1)
      ↓
  全连接层2: 256 → 256
      ↓
  ReLU + Dropout(0.1)
      ↓
  全连接层3: 256 → 1
      ↓
  期望收益分数 (1维标量)
  ```

**为什么是256？**
- **任务复杂度**：价值预测（回归任务）通常比策略预测（分类任务）简单
- **计算效率**：256维隐藏层已经足够学习价值函数
- **经验值**：在AlphaGo Zero等模型中，价值头通常比策略头小

**价值头预测什么？**
- 当前局面的期望最终得分
- 包括：胡牌得分、杠钱、查大叫/查花猪奖惩、最终结算得分
- 用于评估动作的长期价值，指导策略学习

## 数据流详解

### 完整前向传播流程

```
1. 输入处理
   输入: (batch_size, 64, 4, 9)
   - batch_size: 批次大小（训练时通常是8192）
   - 64: 特征平面数
   - 4×9: 每个特征平面的空间维度（4个玩家，9种牌）

2. 骨干网络（ResNet Backbone）
   a. 初始卷积层
      输入: (batch, 64, 4, 9)
      输出: (batch, 128, 4, 9)
      作用: 将64个特征平面转换为128通道的特征图
   
   b. 20个残差块
      输入: (batch, 128, 4, 9)
      处理: 逐步提取深层特征，通道数逐步增加
      输出: (batch, 512, 4, 9)（经过下采样后空间维度可能变化）
      作用: 学习复杂的游戏状态特征表示
   
   c. 全局平均池化
      输入: (batch, 512, H, W)
      输出: (batch, 512, 1, 1)
      作用: 将空间维度压缩为1×1，保留通道信息
   
   d. 全连接层
      输入: (batch, 512)
      输出: (batch, 512)
      作用: 将特征图压缩为特征向量

3. 策略头（Policy Head）
   输入: (batch, 512)
   处理: 3层全连接网络 + Softmax
   输出: (batch, 434)
   作用: 输出每个动作的概率分布

4. 价值头（Value Head）
   输入: (batch, 512)
   处理: 3层全连接网络
   输出: (batch, 1)
   作用: 输出当前局面的期望收益分数
```

## 参数数量估算

### 骨干网络参数

- **初始卷积层**：64 × 128 × 3 × 3 ≈ 73,728
- **20个残差块**：每个块约 100K-500K 参数（取决于通道数）
- **全连接层**：512 × 512 ≈ 262,144
- **总计**：约 5-10M 参数

### 策略头参数

- **全连接层1**：512 × 256 ≈ 131,072
- **全连接层2**：256 × 256 ≈ 65,536
- **全连接层3**：256 × 434 ≈ 111,104
- **总计**：约 300K 参数

### 价值头参数

- **全连接层1**：512 × 256 ≈ 131,072
- **全连接层2**：256 × 256 ≈ 65,536
- **全连接层3**：256 × 1 ≈ 256
- **总计**：约 200K 参数

### 总参数数量

- **总计**：约 6-11M 参数
- **可训练参数**：全部可训练
- **显存占用**（batch_size=8192）：约 2-4GB（取决于具体实现）

## 设计理念

### 1. 为什么使用ResNet？

- **深度优势**：深层网络可以学习更复杂的特征表示
- **残差连接**：解决梯度消失问题，使深层网络可以训练
- **特征重用**：残差连接允许网络学习残差，更容易优化

### 2. 为什么使用双头架构？

- **策略-价值分离**：策略和价值是两个不同的任务，分开学习更有效
- **共享特征**：骨干网络提取的特征被两个头共享，提高效率
- **训练稳定性**：双头架构在强化学习中已被证明非常有效（AlphaGo Zero、AlphaZero）

### 3. 为什么使用卷积网络？

- **空间结构**：麻将牌局有明确的空间结构（4个玩家，9种牌）
- **局部性**：卷积操作可以捕捉局部特征（如某个玩家的手牌分布）
- **参数共享**：卷积层参数共享，减少参数量，提高泛化能力

## 调优建议

### 1. 如果训练速度慢

- **减少 `num_blocks`**：从20减少到15或10
- **减少 `channels`**：从128减少到96或64
- **减少 `batch_size`**：从8192减少到4096或2048

### 2. 如果模型表达能力不足

- **增加 `num_blocks`**：从20增加到30或40
- **增加 `channels`**：从128增加到256
- **增加 `hidden_size`**：策略头从512增加到1024

### 3. 如果显存不足

- **减少 `batch_size`**：从8192减少到4096或2048
- **减少 `num_blocks`**：从20减少到15
- **减少 `channels`**：从128减少到96

### 4. 如果过拟合

- **增加 Dropout**：在策略头和价值头中增加Dropout率
- **增加正则化**：在训练配置中增加权重衰减（weight decay）
- **数据增强**：启用数据增强功能

## 总结

Dual-ResNet 模型架构设计要点：

1. **输入层**（64通道）：编码完整的游戏状态信息
2. **骨干网络**（20层ResNet）：提取深层特征表示
3. **策略头**（512→256→434）：输出动作概率分布
4. **价值头**（512→256→1）：输出期望收益分数

这个架构在**表达能力**、**训练效率**和**计算资源**之间取得了良好的平衡，适合在4x4090 GPU上进行训练。

