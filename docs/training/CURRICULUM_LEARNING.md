# 课程学习完整指南

## 概述

课程学习（Curriculum Learning）是一种分阶段训练策略，让 AI 从简单到复杂逐步学习。系统会自动判断是否应该进入下一阶段，并生成课程规划文档供您手动提交给大模型进行分析。

## 快速开始

### 1. 启用课程学习

在 `config.yaml` 中启用：

```yaml
curriculum_learning:
  enabled: true                # 启用课程学习
  initial_stage: declare_suit  # 初始阶段
  llm_coach_frequency: 100      # 每 100 次迭代生成一次课程规划文档
  document_output_dir: ./coach_documents  # 文档输出目录
  
  # 喂牌机制配置
  feeding_games:
    enabled: true               # 是否启用喂牌机制
    difficulty: easy           # 难度级别（easy/medium/hard）
    feeding_rate: 0.2          # 喂牌概率（20%，即2:8比例：20%喂牌，80%随机）
    win_types:                 # 要学习的胡牌类型
      - basic                  # 基本胡牌
      - seven_pairs            # 七对
      - pure_suit              # 清一色
```

### 2. 运行训练

```bash
python train.py --config config.yaml
```

系统会自动：
- 根据当前阶段设置训练目标
- 定期检查是否满足进入下一阶段的条件
- 生成课程规划文档

---

## 训练阶段详解

### 阶段 1: 定缺阶段 (DECLARE_SUIT)

**目标**: 学习定缺规则和选择

**训练内容**:
- 理解定缺规则
- 学会选择定缺花色
- 避免成为花猪

**评估标准**:
- 花猪率 < 50%（初期非常宽松）
- 定缺选择正确率 ≥ 60%
- 至少完成500局游戏

**迭代次数**:
- 最小: 2000次
- 最大: 8000次（强制推进）

**特点**: 不要求胜率，主要学习规则

---

### 阶段 2: 学胡阶段 (LEARN_WIN) - **喂牌模式**

**目标**: 学习基本胡牌（使用喂牌机制）

**训练内容**:
- 识别基本胡牌类型（平胡、七对等）
- 理解听牌概念
- 学会主动胡牌

**评估标准**:
- 胜率 ≥ 20%（喂牌模式下更容易达到）
- 听牌率 ≥ 30%
- 至少学会2种胡牌类型
- 至少完成1000局游戏

**迭代次数**:
- 最小: 4000次
- 最大: 12000次（强制推进）

**特点**: **启用喂牌模式**，生成更容易学习的牌局

---

### 阶段 3: 基础阶段 (BASIC)

**目标**: 学习定缺和基本胡牌（正常牌局）

**训练内容**:
- 在正常牌局中应用定缺策略
- 在正常牌局中识别胡牌机会
- 理解缺一门规则

**评估标准**:
- 花猪率 < 25%
- 听牌率 ≥ 15%
- 胜率 ≥ 10%（正常牌局下）
- 至少完成1500局游戏

**迭代次数**:
- 最小: 5000次
- 最大: 15000次（强制推进）

**特点**: 使用正常牌局，不再喂牌

---

### 阶段 4: 防御阶段 (DEFENSIVE)

**目标**: 学习避炮策略

**评估标准**:
- 点炮率 < 20%
- 胜率 ≥ 10%
- 听牌率 ≥ 15%

**迭代次数**:
- 最小: 8000次
- 最大: 25000次

---

### 阶段 5: 高级阶段 (ADVANCED)

**目标**: 学习博弈高阶策略

**评估标准**:
- Elo分数 ≥ 1100
- 平均得分 ≥ 3.0
- 胜率 ≥ 20%

**迭代次数**:
- 最小: 15000次
- 最大: 40000次

---

### 阶段 6: 专家阶段 (EXPERT)

**目标**: 学习复杂策略组合

**评估标准**:
- Elo分数 ≥ 1800
- 胜率 ≥ 45%

**迭代次数**:
- 最小: 40000次
- 最大: 无上限

---

## 喂牌机制

### 什么是喂牌？

喂牌是一种训练辅助机制，通过生成更容易学习的牌局，帮助AI在初期更快学会基本技能。

### 喂牌方式

1. **接近听牌的手牌**: 给AI一个接近听牌或已听牌的手牌
2. **牌墙优化**: 在牌墙中放置AI需要的牌
3. **减少威胁**: 减少对手的威胁牌

### 配置

在 `config.yaml` 中配置：

```yaml
curriculum_learning:
  feeding_games:
    enabled: true               # 是否启用喂牌
    difficulty: easy           # 难度级别
    feeding_rate: 0.2          # 喂牌概率（20%，即2:8比例：20%喂牌，80%随机）
    win_types:                 # 要学习的胡牌类型
      - basic                  # 基本胡牌
      - seven_pairs            # 七对
      - pure_suit              # 清一色
```

### 难度级别

- **easy**: 20%的概率生成喂牌局（2:8比例）
- **medium**: 50%的概率
- **hard**: 80%的概率

### 使用阶段

喂牌机制主要在以下阶段使用：
- **学胡阶段 (LEARN_WIN)**: 主要使用喂牌，帮助AI学会胡牌
- **定缺阶段 (DECLARE_SUIT)**: 可选使用，帮助理解规则

其他阶段使用正常牌局。

---

## 0基础数据优化

### 问题

在0基础数据的情况下，随机打牌可能永远不会和牌，导致：
- 胜率可能永远是0或接近0
- 评估标准（如胜率≥30%）永远无法满足
- 课程学习无法推进到下一阶段

### 解决方案

#### 1. 基于迭代次数的自动推进（主要机制）

系统现在**主要依赖迭代次数**进行推进，而不是性能指标：

- **最小迭代次数** (`min_iterations`): 至少训练N次迭代后才考虑推进
- **最大迭代次数** (`max_iterations`): 达到N次迭代后**强制推进**，即使不满足评估标准

**这是0基础数据下的主要推进机制！**

#### 2. 移除或大幅降低胜率要求

基础阶段**不再要求胜率**，因为随机打牌很难达到：

**原始标准**（可能永远无法达到）:
- ❌ 胜率 ≥ 30%
- ❌ 胜率 ≥ 5%

**新标准**（更适合0基础）:
- ✅ **不要求胜率**（基础阶段）
- ✅ 花猪率 < 30%（初期允许很高）
- ✅ 听牌率 ≥ 5%（更容易达到）
- ✅ 至少完成1000局游戏（确保有足够训练）

#### 3. 非常宽松的推进条件

- **基础阶段**: 达到最小迭代次数 + 有任意进步即可推进
- **其他阶段**: 满足30-50%的标准即可推进
- **达到最大迭代次数**: 强制推进（无论指标如何）

---

## 工作流程

### 完整训练流程

```
1. 定缺阶段 (2000-8000次迭代)
   - 学习定缺规则
   - 不要求胜率
   - 正常牌局

2. 学胡阶段 (4000-12000次迭代) ⭐ 喂牌模式
   - 使用喂牌机制
   - 学习基本胡牌
   - 胜率要求: ≥20%（喂牌下更容易）

3. 基础阶段 (5000-15000次迭代)
   - 正常牌局
   - 应用已学技能
   - 胜率要求: ≥10%

4. 防御阶段 (8000-25000次迭代)
   - 学习避炮策略
   - 胜率要求: ≥10%

5. 高级阶段 (15000-40000次迭代)
   - 学习博弈策略
   - Elo要求: ≥1100

6. 专家阶段 (40000+次迭代)
   - 学习复杂策略
   - Elo要求: ≥1800
```

---

## 使用示例

### 启用课程学习和喂牌

```yaml
# config.yaml
curriculum_learning:
  enabled: true
  initial_stage: declare_suit
  feeding_games:
    enabled: true
    difficulty: easy
    feeding_rate: 0.2
```

### 在代码中检查是否使用喂牌

```python
from scai.coach import CurriculumLearning

curriculum = CurriculumLearning()

# 检查当前是否应该使用喂牌
if curriculum.should_use_feeding_games():
    print("当前阶段使用喂牌模式")
    # 生成喂牌局
    from scai.selfplay.feeding_games import FeedingGameGenerator
    generator = FeedingGameGenerator(difficulty='easy')
    feeding_hand = generator.generate_feeding_hand(
        target_player_id=0,
        win_type='basic'
    )
```

---

## 优势

### 1. 渐进式学习

从最简单的定缺规则开始，逐步学习更复杂的技能。

### 2. 喂牌加速学习

在学胡阶段使用喂牌，让AI更快学会基本胡牌，避免长期无法和牌。

### 3. 灵活推进

每个阶段都有最小和最大迭代次数，确保训练能够持续推进。

### 4. 适应0基础数据

初期阶段不要求胜率，主要依赖迭代次数和基本指标推进。

---

## 注意事项

1. **喂牌是训练辅助**: 只在特定阶段使用，最终要过渡到正常牌局
2. **逐步提高难度**: 从喂牌到正常牌局，逐步提高难度
3. **监控学习进度**: 定期检查AI是否真正学会了技能，而不只是依赖喂牌
4. **灵活调整**: 根据实际情况调整喂牌概率和难度

---

## 总结

通过细分阶段和喂牌机制，课程学习现在可以：
- ✅ 从最简单的定缺规则开始
- ✅ 使用喂牌帮助AI学会基本胡牌
- ✅ 逐步过渡到正常牌局
- ✅ 在0基础数据下也能正常工作

这大大提高了训练效率，特别是在初期阶段。

