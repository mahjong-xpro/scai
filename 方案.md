# 血战到底 AI 强化学习方案

## 概述

在完全没有基础数据的情况下，达到人类顶级水平的路径只有一条：**强化学习自对弈（Self-play）**。这套方案模仿了 AlphaZero 和微软 Suphx 的核心逻辑，但针对"血战到底"特有的规则（定缺、刮风下雨、三家胡牌后继续）进行了深度优化。

**技术选型说明**：用 Rust 替代 C++ 是极佳的选择。Rust 的内存安全特性（Memory Safety）能有效避免麻将逻辑中复杂的指针错误，且其并发性能（Zero-cost Abstractions）非常适合大规模强化学习的训练数据生成。在麻将模拟中，由于每局要处理大量的分支判断（碰、杠、胡优先级），Python 的运行效率极其低下。Rust 的 match 模式匹配和底层执行效率，能让你在同样的硬件上多跑 50-100 倍的对局数据，这是零数据起步的关键。

---

## 一、核心技术栈

### 1.1 模拟引擎 (Environment)

- **实现方式**：高性能 Rust 模拟器
- **职责**：
  - 执行规则
  - 状态转换
  - 奖励计算
- **技术选型说明**：
  - Rust 性能与 C++ 相当，在并发场景下可能更优
  - 内存安全性更好，减少并发环境下的数据竞争和内存泄漏
  - 通过 PyO3 与 Python 训练框架无缝集成
  - 优秀的并发模型（async/await + tokio）适合高并发自对弈场景

### 1.2 通信与并行

- **Ray**：分布式管理框架，用于开启几千个并发进程
- **Tokio**：Rust 异步并发框架，用于 Rust 端的并发处理

### 1.3 深度学习框架

- **PyTorch**：用于神经网络训练（推荐）
- **JAX**：替代选择
- **Burn**：Rust 原生深度学习框架（可选）

### 1.4 算法

- **PPO (Proximal Policy Optimization)**：基础强化学习算法
- **ISMCTS (信息集蒙特卡洛搜索)**：用于决策搜索
- **Oracle Guiding (先知引导)**：训练阶段让 AI 看到全场明牌
- **Global Reward Prediction (全局奖励预测)**：预测这一轮结束后的最终总分

---

## 二、网络架构

### 2.1 架构设计

- **架构**：Dual-ResNet
- **结构**：
  - **Backbone**：20 个残差块 (Residual Blocks)，每块包含 Conv2d -> BatchNorm -> ReLU -> Conv2d
  - 一个共享的残差骨架提取特征
  - 分支两个头：
    - **Policy Head (策略头)**：输出 100+ 维度的概率分布，代表所有可能的出牌和动作
    - **Value Head (价值头)**：预测当前局面的期望得分（在血战中，这个得分是"最终总分"的回归值）

---

## 三、具体实现过程

### 阶段 1：Rust 核心引擎开发 (The Engine)

**任务**：用 Rust 实现血战到底引擎，负责处理高频的逻辑计算。

**实现要点**：

1. **牌组与动作定义**
   - 使用 Rust 的 Enum 和 BitSet 来极大化处理速度
   - 使用位运算存储牌墙，极大提升速度
   ```rust
   pub enum Tile { Wan(u8), Tong(u8), Tiao(u8) }
   pub enum Action { Discard(u8), Chi(u8), Peng(u8), Gang(u8), Hu, Pass }
   ```

2. **状态转移**
   - 实现 `step` 函数
   - 在血战到底中，一家胡牌后，将该玩家标记为 `Finished`
   - `GameState` 持续直到剩下一人或牌抓完

3. **动作空间映射 (Action Space)**
   - 定义 108 个出牌动作
   - 11 个交互动作（定缺 3 门、碰、杠、自摸、点炮胡、跳过）

4. **合法动作掩码 (Action Masking)**
   - Rust 需返回一个 `Vec<bool>`
   - 若玩家未打完定缺色，则所有非该色牌的操作均设为 `false`
   - 这是零数据成功的关键

5. **奖励塑造 (Reward Shaping)**
   - 初始阶段不要只看最后输赢（太稀疏）
   - 给"听牌"设置微小的正向奖励 (+0.1)
   - 给"胡牌"设置中等奖励 (+1.0)
   - 给"花猪"设置重罚 (-5.0)

6. **PyO3 绑定**
   - 完成血战引擎，支持 PyO3 绑定
   - 将 Rust 中的游戏数据暴露给 Python 训练框架

### 阶段 2：状态张量化 (State Representation)

**任务**：将 Rust 中的游戏数据转化为神经网络能理解的 4D Tensor。

**特征图设计 (64×4×9)**：

- **手牌层**：4 个平面，分别代表自己拥有某张牌的数量（1-4）
- **定缺层**：3 个平面，标记各家定缺状态
- **弃牌/副露层**：记录三名对手已打出或碰杠的牌
- **历史信息**：采用过去两轮的动作记录，捕捉对手可能的听牌方向

### 阶段 3：神经网络架构 (The Brain)

**任务**：在 Python 端使用 PyTorch 构建 Dual-ResNet。

**关键要点**：

- **Backbone**：20 个残差块，提取特征
- **Policy Head**：输出所有可能动作的概率分布
- **Value Head**：预测当前局面的期望得分（最终总分）

### 阶段 4：Oracle-Guiding 训练 (Learning the Invisible)

由于看不到对手的手牌，模型初级阶段会非常困惑。

**具体过程**：

- **训练时**：
  - 给 Critic（价值头）额外输入"全场明牌"特征
  - 让模型知道"为什么我刚才那张牌会点炮"，从而加速收敛

- **推理时**：
  - 去掉这些完美信息，只给模型输入已知的弃牌和自己的手牌
  - 这类似于"带着透视挂训练，关掉透视挂考试"

### 阶段 5：自对弈强化学习流程 (The Learning Loop)

这是零基础达到顶级的核心过程：

1. **初始化**
   - 生成随机权重的神经网络 $M_0$

2. **数据采集 (Rust Parallelism)**
   - 启动数千个 Rust 线程实例
   - 让四个 $M_0$ 相互博弈

3. **优化**
   - 使用 PPO 算法更新权重
   - 最大化最终结算得分（包括刮风下雨和查大叫的分数）

4. **评估与迭代**
   - 每训练 1 小时，让新模型 $M_{new}$ 与旧模型 $M_{old}$ 对打
   - 采用 Elo 评分系统，只有胜率达标的模型才会被选作下一轮的数据生成器

### 阶段 6：血战到底专项策略优化

血战到底的精髓在于"一家胡牌，三家继续"。AI 必须学会价值平衡。

**实现过程**：

1. **多局博弈价值分析**
   - 修改 Value Head，让它不仅预测这一手的输赢
   - 还预测场上其他玩家胡牌后对自己的潜在威胁

2. **贪婪博弈逻辑（期望收益计算）**
   - 在 Rust 引擎中实现一个简单的递归评估
   - 当 AI 摸到胡牌时，模型会输出两个价值：
     - $V_{hu}$：现在胡牌的确定收益
     - $V_{continue}$：继续打球博大番的期望收益
   - 将这两者的差值作为强化学习的额外奖励信号（Advantage）
   - 以此训练 AI 何时应该"见好就收"，何时应该"血战到底"

### 阶段 7：规模化自对弈 (Self-Play Evolution)

**架构部署**：

- 使用 Ray 框架，开启几千个并发进程

**Elo 系统**：

- 每隔 10,000 次迭代生成一个 Snapshot
- 让最新版本的 AI 与 24 小时前的 AI 对打
- 如果胜率超过 55%，则保留新模型

**探索因子**：

- 在动作输出层加入 Dirichlet Noise (狄利克雷噪声)
- 防止 AI 每次都打同一套路
- 鼓励它发现冷门打法（如：极早期的清一色）

---

## 四、开发路线图与里程碑

| 周期 | 模块 | 实现目标 |
|------|------|----------|
| W1 | Rust Core | 完成血战引擎，支持 PyO3 绑定 |
| W2 | Tensor Encoder | 完成牌局状态到 $N \times 4 \times 9$ 张量的转换 |
| W3-4 | PPO Training | 启动 100 万局自对弈，AI 应学会定缺并能正确胡牌 |
| W5-6 | Oracle Critic | 引入"上帝视角"训练，AI 开始学会防守和算牌 |
| W7+ | Scale Up | 增加算力，通过长达数周的对弈进化出人类顶级策略 |

**详细时间表**：

| 时间 | 目标 | 预期结果 |
|------|------|----------|
| 第 1-2 周 | 环境开发 | 模拟器每秒可跑 1 万局随机对局 |
| 第 3-4 周 | 基础收敛 | AI 学会了定缺、凑顺子对子、并能主动胡牌 |
| 第 5-8 周 | 深度博弈 | 引入 Oracle 引导。AI 学会了计算对手的缺门，开始避开明显的点炮 |
| 第 9 周后 | 人类顶级 | 算力堆叠。AI 能够根据场上已胡牌的人数，动态切换进攻/防御策略 |

---

## 五、硬件与算力建议

由于是从 0 开始，至少需要：

1. **1 块 NVIDIA RTX 4090 (或以上)**
   - 用于神经网络梯度更新

2. **128 核以上 CPU (多路服务器)**
   - 用于高并发运行模拟环境产生数据

---

## 六、开发建议

### 6.1 为什么用 Rust？

在麻将模拟中，由于每局要处理大量的分支判断（碰、杠、胡优先级），Python 的运行效率极其低下。Rust 的 match 模式匹配和底层执行效率，能让你在同样的硬件上多跑 50-100 倍的对局数据，这是零数据起步的关键。

### 6.2 技术优势

- **内存安全**：避免复杂的指针错误
- **并发性能**：Zero-cost Abstractions，适合大规模强化学习训练数据生成
- **执行效率**：比 Python 快 50-100 倍
- **模式匹配**：Rust 的 match 语句非常适合游戏逻辑的状态判断

---

## 下一步

我们可以先确定状态编码（State Representation）的具体维度。

**选择方向**：
- 先从 Python 的基础训练代码框架写起
- 或先定义 Rust 环境的接口逻辑（通过 PyO3 暴露 Python 绑定）

**建议**：您是否需要我为您展示如何使用 PyO3 将 Rust 的牌局状态传递给 Python 的核心代码骨架？
