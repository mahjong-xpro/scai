# 课程学习：0基础数据优化说明

## 问题

在0基础数据的情况下，随机打牌可能永远不会和牌，导致：
- 胜率可能永远是0或接近0
- 评估标准（如胜率≥30%）永远无法满足
- 课程学习无法推进到下一阶段

## 解决方案

### 1. 基于迭代次数的自动推进（主要机制）

系统现在**主要依赖迭代次数**进行推进，而不是性能指标：

- **最小迭代次数** (`min_iterations`): 至少训练N次迭代后才考虑推进
- **最大迭代次数** (`max_iterations`): 达到N次迭代后**强制推进**，即使不满足评估标准

**这是0基础数据下的主要推进机制！**

### 2. 移除或大幅降低胜率要求

基础阶段**不再要求胜率**，因为随机打牌很难达到：

**原始标准**（可能永远无法达到）:
- ❌ 胜率 ≥ 30%
- ❌ 胜率 ≥ 5%

**新标准**（更适合0基础）:
- ✅ **不要求胜率**（基础阶段）
- ✅ 花猪率 < 30%（初期允许很高）
- ✅ 听牌率 ≥ 5%（更容易达到）
- ✅ 至少完成1000局游戏（确保有足够训练）

### 3. 非常宽松的推进条件

- **基础阶段**: 达到最小迭代次数 + 有任意进步即可推进
- **其他阶段**: 满足30-50%的标准即可推进
- **达到最大迭代次数**: 强制推进（无论指标如何）

## 阶段配置

### 基础阶段 (BASIC) - 最宽松

```python
evaluation_criteria={
    # 'win_rate': 0.05,  # ❌ 移除胜率要求
    'flower_pig_rate': 0.30,   # 花猪率低于30%（非常宽松）
    'ready_rate': 0.05,        # 听牌率至少5%（只要有进步即可）
    'games_played': 1000,      # 至少完成1000局游戏
}
min_iterations=3000            # 至少训练3000次（降低门槛）
max_iterations=12000           # 最多12000次后强制推进（更激进）
```

**推进条件**（满足任一即可）:
1. 达到最小迭代次数（3000次）**且**有任何进步（如听牌率>0）
2. 满足至少30%的标准
3. **达到最大迭代次数（12000次）→ 强制推进**

### 防御阶段 (DEFENSIVE)

```python
evaluation_criteria={
    'discard_win_rate': 0.20,  # 点炮率低于20%
    'win_rate': 0.10,          # 胜率至少10%（降低要求）
    'ready_rate': 0.15,        # 听牌率至少15%
}
min_iterations=8000
max_iterations=25000
```

### 高级阶段 (ADVANCED)

```python
evaluation_criteria={
    'elo_score': 1100,         # Elo分数至少1100（进一步降低）
    'average_score': 3.0,      # 平均得分至少3分（进一步降低）
    'win_rate': 0.20,          # 胜率至少20%（降低要求）
}
min_iterations=15000
max_iterations=40000
```

### 专家阶段 (EXPERT)

```python
evaluation_criteria={
    'elo_score': 1800,         # Elo分数至少1800
    'win_rate': 0.45,          # 胜率至少45%
}
min_iterations=40000
max_iterations=0              # 不设上限
```

## 工作流程（0基础数据）

### 场景1: 随机打牌，胜率永远是0%

```
迭代 0-3000:   训练中，不检查推进条件
迭代 3000:     达到最小迭代次数
  - 胜率: 0% (不要求)
  - 花猪率: 35% (需要<30%，未满足)
  - 听牌率: 3% (需要≥5%，未满足)
  - 游戏数: 500 (需要≥1000，未满足)
  → 继续训练（有进步但未满足标准）

迭代 5000:     继续训练
  - 听牌率: 6% (满足≥5%)
  - 游戏数: 1200 (满足≥1000)
  → 满足30%标准 → 推进到防御阶段 ✅

或者

迭代 12000:    达到最大迭代次数
  - 即使所有指标都不满足
  → 强制推进到防御阶段 ✅
```

### 场景2: 完全随机，没有任何进步

```
迭代 0-3000:   训练中
迭代 3000-12000: 训练中，指标没有改善
迭代 12000:    达到最大迭代次数
  → 强制推进到防御阶段 ✅
  → 继续训练，逐步改善
```

## 关键改进

### 1. 基础阶段不要求胜率

```python
# 移除胜率要求
evaluation_criteria={
    # 'win_rate': 0.05,  # ❌ 不再要求
    'ready_rate': 0.05,  # ✅ 更容易达到
    'games_played': 1000,  # ✅ 确保有足够训练
}
```

### 2. 更激进的迭代推进

```python
min_iterations=3000   # 降低到3000次
max_iterations=12000  # 降低到12000次（更早强制推进）
```

### 3. 非常宽松的推进条件

```python
# 基础阶段：达到最小迭代次数 + 有任意进步即可
if current_iteration >= min_iterations and met_criteria > 0:
    return True  # 推进！

# 或者满足30%标准即可（而不是50%）
if met_criteria >= total_criteria * 0.3:
    return True
```

## 使用建议

### 1. 初期训练（0-12000次迭代）

- **不要担心胜率**: 系统不要求胜率，主要看迭代次数
- **关注其他指标**: 听牌率、游戏数量等更容易达到
- **信任自动推进**: 系统会在12000次迭代后强制推进

### 2. 监控训练进度

```python
# 检查当前阶段和迭代次数
current = curriculum.get_current_curriculum()
print(f"当前阶段: {current.name}")
print(f"已训练: {current_iteration} 次")
print(f"最小迭代: {current.min_iterations}")
print(f"最大迭代: {current.max_iterations}")

# 检查是否接近强制推进
if current_iteration >= current.max_iterations * 0.9:
    print("警告: 接近最大迭代次数，即将强制推进")
```

### 3. 如果推进太慢

可以进一步降低迭代次数要求：

```python
# 在 curriculum.py 中修改
min_iterations=2000   # 进一步降低
max_iterations=10000  # 进一步降低
```

## 总结

**核心思想**: 在0基础数据下，**主要依赖迭代次数推进**，而不是性能指标。

1. ✅ **基础阶段不要求胜率** - 随机打牌很难达到
2. ✅ **更激进的迭代推进** - 3000-12000次即可推进
3. ✅ **非常宽松的推进条件** - 有任意进步即可推进
4. ✅ **强制推进机制** - 达到最大迭代次数后强制推进

这样即使胜率永远是0%，系统也能正常推进，确保训练能够持续进行。
