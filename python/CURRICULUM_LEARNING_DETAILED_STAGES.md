# 课程学习：细分阶段和喂牌机制

## 概述

课程学习现在被细分为更多阶段，并支持"喂牌"机制，帮助AI在0基础数据下更快学会基本技能。

## 训练阶段（从简单到复杂）

### 阶段 1: 定缺阶段 (DECLARE_SUIT)

**目标**: 学习定缺规则和选择

**训练内容**:
- 理解定缺规则
- 学会选择定缺花色
- 避免成为花猪

**评估标准**:
- 花猪率 < 50%（初期非常宽松）
- 定缺选择正确率 ≥ 60%
- 至少完成500局游戏

**迭代次数**:
- 最小: 2000次
- 最大: 8000次（强制推进）

**特点**: 不要求胜率，主要学习规则

---

### 阶段 2: 学胡阶段 (LEARN_WIN) - **喂牌模式**

**目标**: 学习基本胡牌（使用喂牌机制）

**训练内容**:
- 识别基本胡牌类型（平胡、七对等）
- 理解听牌概念
- 学会主动胡牌

**评估标准**:
- 胜率 ≥ 20%（喂牌模式下更容易达到）
- 听牌率 ≥ 30%
- 至少学会2种胡牌类型
- 至少完成1000局游戏

**迭代次数**:
- 最小: 4000次
- 最大: 12000次（强制推进）

**特点**: **启用喂牌模式**，生成更容易学习的牌局

---

### 阶段 3: 基础阶段 (BASIC)

**目标**: 学习定缺和基本胡牌（正常牌局）

**训练内容**:
- 在正常牌局中应用定缺策略
- 在正常牌局中识别胡牌机会
- 理解缺一门规则

**评估标准**:
- 花猪率 < 25%
- 听牌率 ≥ 15%
- 胜率 ≥ 10%（正常牌局下）
- 至少完成1500局游戏

**迭代次数**:
- 最小: 5000次
- 最大: 15000次（强制推进）

**特点**: 使用正常牌局，不再喂牌

---

### 阶段 4: 防御阶段 (DEFENSIVE)

**目标**: 学习避炮策略

**评估标准**:
- 点炮率 < 20%
- 胜率 ≥ 10%
- 听牌率 ≥ 15%

**迭代次数**:
- 最小: 8000次
- 最大: 25000次

---

### 阶段 5: 高级阶段 (ADVANCED)

**目标**: 学习博弈高阶策略

**评估标准**:
- Elo分数 ≥ 1100
- 平均得分 ≥ 3.0
- 胜率 ≥ 20%

**迭代次数**:
- 最小: 15000次
- 最大: 40000次

---

### 阶段 6: 专家阶段 (EXPERT)

**目标**: 学习复杂策略组合

**评估标准**:
- Elo分数 ≥ 1800
- 胜率 ≥ 45%

**迭代次数**:
- 最小: 40000次
- 最大: 无上限

---

## 喂牌机制

### 什么是喂牌？

喂牌是一种训练辅助机制，通过生成更容易学习的牌局，帮助AI在初期更快学会基本技能。

### 喂牌方式

1. **接近听牌的手牌**: 给AI一个接近听牌或已听牌的手牌
2. **牌墙优化**: 在牌墙中放置AI需要的牌
3. **减少威胁**: 减少对手的威胁牌

### 配置

在 `config.yaml` 中配置：

```yaml
curriculum_learning:
  feeding_games:
    enabled: true               # 是否启用喂牌
    difficulty: easy           # 难度级别
    feeding_rate: 0.2          # 喂牌概率（20%，即2:8比例：20%喂牌，80%随机）
    win_types:                 # 要学习的胡牌类型
      - basic                  # 基本胡牌
      - seven_pairs            # 七对
      - pure_suit              # 清一色
```

**注意**: 默认配置为 2:8 比例（20% 喂牌，80% 随机），这样可以：
- 让AI在喂牌局中学习基本技能
- 在随机局中验证和巩固学习成果
- 避免过度依赖喂牌，保持训练的多样性

### 难度级别

- **easy**: 80%的概率生成喂牌局
- **medium**: 50%的概率
- **hard**: 20%的概率

### 使用阶段

喂牌机制主要在以下阶段使用：
- **学胡阶段 (LEARN_WIN)**: 主要使用喂牌，帮助AI学会胡牌
- **定缺阶段 (DECLARE_SUIT)**: 可选使用，帮助理解规则

其他阶段使用正常牌局。

## 工作流程

### 完整训练流程

```
1. 定缺阶段 (2000-8000次迭代)
   - 学习定缺规则
   - 不要求胜率
   - 正常牌局

2. 学胡阶段 (4000-12000次迭代) ⭐ 喂牌模式
   - 使用喂牌机制
   - 学习基本胡牌
   - 胜率要求: ≥20%（喂牌下更容易）

3. 基础阶段 (5000-15000次迭代)
   - 正常牌局
   - 应用已学技能
   - 胜率要求: ≥10%

4. 防御阶段 (8000-25000次迭代)
   - 学习避炮策略
   - 胜率要求: ≥10%

5. 高级阶段 (15000-40000次迭代)
   - 学习博弈策略
   - Elo要求: ≥1100

6. 专家阶段 (40000+次迭代)
   - 学习复杂策略
   - Elo要求: ≥1800
```

## 使用示例

### 启用课程学习和喂牌

```yaml
# config.yaml
curriculum_learning:
  enabled: true
  initial_stage: declare_suit
  feeding_games:
    enabled: true
    difficulty: easy
    feeding_rate: 0.8
```

### 在代码中检查是否使用喂牌

```python
from scai.coach import CurriculumLearning

curriculum = CurriculumLearning()

# 检查当前是否应该使用喂牌
if curriculum.should_use_feeding_games():
    print("当前阶段使用喂牌模式")
    # 生成喂牌局
    from scai.selfplay.feeding_games import FeedingGameGenerator
    generator = FeedingGameGenerator(difficulty='easy')
    feeding_hand = generator.generate_feeding_hand(
        target_player_id=0,
        win_type='basic'
    )
```

## 优势

### 1. 渐进式学习

从最简单的定缺规则开始，逐步学习更复杂的技能。

### 2. 喂牌加速学习

在学胡阶段使用喂牌，让AI更快学会基本胡牌，避免长期无法和牌。

### 3. 灵活推进

每个阶段都有最小和最大迭代次数，确保训练能够持续推进。

### 4. 适应0基础数据

初期阶段不要求胜率，主要依赖迭代次数和基本指标推进。

## 注意事项

1. **喂牌是训练辅助**: 只在特定阶段使用，最终要过渡到正常牌局
2. **逐步提高难度**: 从喂牌到正常牌局，逐步提高难度
3. **监控学习进度**: 定期检查AI是否真正学会了技能，而不只是依赖喂牌
4. **灵活调整**: 根据实际情况调整喂牌概率和难度

## 总结

通过细分阶段和喂牌机制，课程学习现在可以：
- ✅ 从最简单的定缺规则开始
- ✅ 使用喂牌帮助AI学会基本胡牌
- ✅ 逐步过渡到正常牌局
- ✅ 在0基础数据下也能正常工作

这大大提高了训练效率，特别是在初期阶段。

